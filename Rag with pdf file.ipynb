{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e16247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import faiss\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models.base import BaseChatOpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import tempfile\n",
    "from pypdf.errors import EmptyFileError\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Streamlit Page Configuration\n",
    "st.set_page_config(page_title=\"üìÑ PDF Chatbot\", layout=\"wide\")\n",
    "\n",
    "# Initialize session state\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state[\"chat_history\"] = []\n",
    "\n",
    "if \"uploaded_file\" not in st.session_state:\n",
    "    st.session_state[\"uploaded_file\"] = None\n",
    "\n",
    "# ---- Sidebar for File Selection ----\n",
    "st.sidebar.header(\"üìÇ Select Your PDF\")\n",
    "selected_file = st.sidebar.file_uploader(\"Choose a PDF\", type=\"pdf\")\n",
    "\n",
    "if st.sidebar.button(\"Upload File\"):\n",
    "    if selected_file:\n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
    "                tmp_file.write(selected_file.read())\n",
    "                pdf_path = tmp_file.name  \n",
    "\n",
    "            if os.stat(pdf_path).st_size == 0:\n",
    "                raise EmptyFileError(\"The uploaded PDF is empty.\")\n",
    "            \n",
    "            st.session_state[\"uploaded_file\"] = pdf_path\n",
    "            st.sidebar.success(\"‚úÖ File uploaded successfully!\")\n",
    "\n",
    "        except EmptyFileError as e:\n",
    "            st.sidebar.error(f\"‚ö†Ô∏è Error: {e}\")\n",
    "            st.session_state[\"uploaded_file\"] = None\n",
    "        except Exception as e:\n",
    "            st.sidebar.error(f\"‚ö†Ô∏è Unexpected Error: {e}\")\n",
    "            st.session_state[\"uploaded_file\"] = None\n",
    "    else:\n",
    "        st.sidebar.warning(\"‚ö†Ô∏è Please select a file first.\")\n",
    "\n",
    "st.sidebar.markdown(\"---\")\n",
    "\n",
    "# ---- Chat UI ----\n",
    "col1, col2 = st.columns([4, 1])  # Make chat area larger\n",
    "\n",
    "with col1:\n",
    "    st.subheader(\"üí¨ Chat with Your PDF\")\n",
    "    chat_container = st.container()\n",
    "\n",
    "    # Display chat history\n",
    "    for chat in st.session_state[\"chat_history\"]:\n",
    "        with chat_container:\n",
    "            st.markdown(f\"**üßë‚Äçüíª You:** {chat['question']}\")\n",
    "            st.markdown(f\"{chat['thinking']}\")\n",
    "            st.markdown(f\"*{chat['answer']}\")\n",
    "\n",
    "    # Function to handle new questions\n",
    "    def handle_new_question():\n",
    "        if st.session_state.user_query.strip():\n",
    "            st.session_state[\"chat_history\"].append({\n",
    "                \"question\": st.session_state.user_query.strip(),\n",
    "                \"thinking\": \"Thinking...\",\n",
    "                \"answer\": \"...\"\n",
    "            })\n",
    "            st.session_state.user_query = \"\"  # Clear input box after submission\n",
    "\n",
    "    # Input box with Enter-to-submit and clear\n",
    "    user_query = st.text_input(\n",
    "        \"üìù Ask a question about the document\",\n",
    "        key=\"user_query\",\n",
    "        on_change=handle_new_question\n",
    "    )\n",
    "\n",
    "# ---- Process User Question ----\n",
    "if st.session_state[\"chat_history\"] and st.session_state[\"chat_history\"][-1][\"thinking\"] == \"Thinking...\":\n",
    "    latest_chat = st.session_state[\"chat_history\"][-1]\n",
    "    user_question = latest_chat[\"question\"]\n",
    "\n",
    "    if st.session_state[\"uploaded_file\"]:\n",
    "        pdf_path = st.session_state[\"uploaded_file\"]\n",
    "\n",
    "        # Load and process PDF\n",
    "        def load_pdf(pdf_path):\n",
    "            try:\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                pages = loader.load()\n",
    "                text = \" \".join([page.page_content for page in pages])\n",
    "                return text\n",
    "            except Exception as e:\n",
    "                st.error(f\"‚ö†Ô∏è Failed to process PDF: {e}\")\n",
    "                return \"\"\n",
    "\n",
    "        pdf_text = load_pdf(pdf_path)\n",
    "\n",
    "        if not pdf_text.strip():\n",
    "            st.error(\"‚ö†Ô∏è The PDF seems to be empty or could not be read.\")\n",
    "        else:\n",
    "            # Split text into chunks\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "            chunks = splitter.split_text(pdf_text)\n",
    "\n",
    "            # Generate embeddings\n",
    "            model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "            embeddings = model.encode(chunks)\n",
    "            embeddings = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "            # Store in FAISS index\n",
    "            index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "            index.add(embeddings)\n",
    "            faiss.write_index(index, \"pdf_index.faiss\")\n",
    "\n",
    "            # Save chunks for later retrieval\n",
    "            with open(\"pdf_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(chunks, f)\n",
    "\n",
    "            # Load FAISS index & model\n",
    "            index = faiss.read_index(\"pdf_index.faiss\")\n",
    "            embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "            # Load chunks\n",
    "            with open(\"pdf_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                chunks = json.load(f)\n",
    "\n",
    "            # LangChain LLM\n",
    "            llm = BaseChatOpenAI(\n",
    "                model=,
    "                api_key=,
    "                base_url=,
    "            )\n",
    "\n",
    "            # Retrieve relevant content\n",
    "            def retrieve_text(query, top_k=3):\n",
    "                query_embedding = embedding_model.encode([query]).astype(np.float32)\n",
    "                distances, indices = index.search(query_embedding, top_k)\n",
    "                retrieved_text = \" \".join([chunks[i] for i in indices[0]])\n",
    "                return retrieved_text\n",
    "\n",
    "            # Generate response with LLM\n",
    "            def chatbot(query):\n",
    "                relevant_text = retrieve_text(query)\n",
    "                response = llm.invoke(f\"Context: {relevant_text}\\n\\nQuery: {query}\\n\\nProvide a detailed reasoning followed by a clear conclusion.\")\n",
    "                return response.content\n",
    "\n",
    "            # Typing effect for response without using rerun\n",
    "            def type_response(thinking, conclusion):\n",
    "                chat_placeholder = st.empty()\n",
    "                display_thinking = \"\"\n",
    "                display_conclusion = \"\"\n",
    "\n",
    "                # Display \"Thinking...\" first, letter by letter\n",
    "                for char in thinking:\n",
    "                    display_thinking += char\n",
    "                    time.sleep(0.02)\n",
    "                    chat_placeholder.markdown(f\"**ü§ñ Thinking:** {display_thinking}\")\n",
    "\n",
    "                # Small pause before the conclusion\n",
    "                time.sleep(1)\n",
    "\n",
    "                # Display \"Conclusion:\" letter by letter\n",
    "                for char in conclusion:\n",
    "                    display_conclusion += char\n",
    "                    time.sleep(0.02)\n",
    "                    chat_placeholder.markdown(f\"**ü§ñ Thinking:** {display_thinking}\\n\\n**‚úÖ Conclusion:** {display_conclusion}\")\n",
    "\n",
    "            # Fetch answer\n",
    "            bot_answer = chatbot(user_question)\n",
    "\n",
    "            # Splitting into Thinking & Conclusion\n",
    "            if \"Conclusion:\" in bot_answer:\n",
    "                thinking_part, conclusion_part = bot_answer.split(\"Conclusion:\", 1)\n",
    "                thinking_part = thinking_part.strip()\n",
    "                conclusion_part = conclusion_part.strip()\n",
    "            else:\n",
    "                thinking_part = bot_answer.strip()\n",
    "                conclusion_part = \"No clear conclusion provided.\"\n",
    "\n",
    "            # Display letter-by-letter response directly\n",
    "            type_response(thinking_part, conclusion_part)\n",
    "\n",
    "            # Update chat history after full response is displayed\n",
    "            latest_chat[\"thinking\"] = thinking_part\n",
    "            latest_chat[\"answer\"] = conclusion_part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
